{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TubesB_13519096",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tugas Besar B - IF3270 Pembelajaran Mesin**\n",
        "Authors:\n",
        "1. 13519096 Girvin Junod\n",
        "2. 13519116 Jeane Mikha Erwansyah\n",
        "3. 13519131 Hera Shafira\n",
        "4. 13519188 Jeremia Axel\n",
        "---"
      ],
      "metadata": {
        "id": "wX_gwSZ_6ZKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "tHAq5ADxPg6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icecream\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-uXAhZ5Pt6V",
        "outputId": "436bbc2a-efd4-4d85-f263-ae14a9cdc768"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: icecream in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.7/dist-packages (from icecream) (0.4.4)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from icecream) (0.8.3)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream) (2.6.1)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from icecream) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load libraries"
      ],
      "metadata": {
        "id": "B6_VuJbZ64yD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tP_mwtuv4W_7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os, subprocess, sys\n",
        "import json, math, typing, copy\n",
        "import numpy as np, networkx as nx, matplotlib as plt\n",
        "from icecream import ic\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enums"
      ],
      "metadata": {
        "id": "QNnoGGyB7W42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerType:\n",
        "\tINPUT = \"input\"\n",
        "\tHIDDEN = \"hidden\"\n",
        "\tOUTPUT = \"output\"\n",
        "\n",
        "class ActivationFunction:\n",
        "\tSIGMOID = \"sigmoid\"\n",
        "\tRELU = \"relu\"\n",
        "\tSOFTMAX = \"softmax\"\n",
        "\tLINEAR = \"linear\""
      ],
      "metadata": {
        "id": "D8Qalm1u7Zl-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "rGxBGS587e6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Utils:\n",
        "\t@staticmethod\n",
        "\tdef matrix_dimension(mat: typing.List[list]) -> typing.Tuple[int, int]:\n",
        "\t\treturn len(mat), len(mat[0])\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_layer_type(type: str):\n",
        "\t\tif type == 'input':\n",
        "\t\t\treturn LayerType.INPUT\n",
        "\t\telif type == 'hidden':\n",
        "\t\t\treturn LayerType.HIDDEN\n",
        "\t\telif type == 'output':\n",
        "\t\t\treturn LayerType.OUTPUT\n",
        "\t  \n",
        "\t@staticmethod\n",
        "\tdef get_activation_func(activation_func: str):\n",
        "\t\tif activation_func == 'sigmoid':\n",
        "\t\t  return ActivationFunction.SIGMOID\n",
        "\t\telif activation_func == 'linear':\n",
        "\t\t  return ActivationFunction.LINEAR\n",
        "\t\telif activation_func == 'relu':\n",
        "\t\t  return ActivationFunction.RELU\n",
        "\t\telif activation_func == 'softmax':\n",
        "\t\t  return ActivationFunction.SOFTMAX\n",
        "\t\t  \n",
        "\t@staticmethod\n",
        "\tdef parse_json(filename):\n",
        "\t\twith open(filename, 'r') as f:\n",
        "\t\t\tdata = json.load(f)\n",
        "\t\t\treturn data\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef export_json(filename, data):\n",
        "\t\twith open(filename, 'w') as f:\n",
        "\t\t\tf.write(json.dumps(data, indent=2))\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef install(package):\n",
        "\t\tsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", ''.join(package)])"
      ],
      "metadata": {
        "id": "ztAH1zztGkcj"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activations:\n",
        "  @staticmethod\n",
        "  def sigmoid(x: np.ndarray):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  @staticmethod\n",
        "  def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "  @staticmethod\n",
        "  def linear(x):\n",
        "    return x\n",
        "  @staticmethod\n",
        "  def softmax(x: np.ndarray):\n",
        "    e_x = np.exp(x-np.max(x))\n",
        "    return e_x/e_x.sum(axis=1).reshape(-1,1)\n",
        "  @staticmethod\n",
        "  def d_sigmoid(x):\n",
        "    return Activations.sigmoid(x) * (1 - Activations.sigmoid(x))\n",
        "  @staticmethod\n",
        "  def d_linear(x):\n",
        "    return 1\n",
        "  @staticmethod\n",
        "  def d_relu(x):\n",
        "    return (x>=0)*1\n",
        "  @staticmethod\n",
        "  def d_softmax(x, y):\n",
        "    de_dnet = x\n",
        "    y_flat = y.flatten()\n",
        "    de_dnet[np.arange(y_flat.shape[0]), y_flat] = -(1-de_dnet[np.arange(y_flat.shape[0]), y_flat])\n",
        "    de_dnet = -de_dnet\n",
        "    return de_dnet\n"
      ],
      "metadata": {
        "id": "7xxTZUlloj6H"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Class"
      ],
      "metadata": {
        "id": "KRyyj_QlGqfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\tdef __init__(self, prev_nodes: int, num_nodes: int, activation_func):\n",
        "\t\tself.weights = np.random.standard_normal((prev_nodes, num_nodes))\n",
        "\t\tself.weights = np.r_[np.zeros((1, num_nodes)), self.weights]\n",
        "\t\tself.num_nodes = num_nodes\n",
        "\t\tself.activation_func = activation_func\n",
        "\t\t\n",
        "\tdef get_params(self):\n",
        "\t\treturn self.weights.size\n",
        "\n",
        "\tdef forward(self, inputs: np.array):\n",
        "\t\tself.last_input = np.c_[np.ones((inputs.shape[0], 1)), inputs]\n",
        "\t\tself.last_unactivated = self.last_input @ self.weights\n",
        "\t\tif self.activation_func == ActivationFunction.SIGMOID:\n",
        "\t \t\tself.last_output = Activations.sigmoid(self.last_unactivated)\n",
        "\t\telif self.activation_func == ActivationFunction.RELU:\n",
        "\t\t\tself.last_output = Activations.relu(self.last_unactivated)\n",
        "\t\telif self.activation_func == ActivationFunction.LINEAR:\n",
        "\t\t\tself.last_output = Activations.linear(self.last_unactivated)\n",
        "\t\telif self.activation_func == ActivationFunction.SOFTMAX:\n",
        "\t\t\tself.last_output = Activations.softmax(self.last_unactivated)\n",
        "\t\treturn self.last_output\n",
        "\t\n",
        "\tdef backpropagate(self, delta: np.ndarray, lr: float, y: np.ndarray = None):\n",
        "\t\tif y is not None:\n",
        "\t\t\tif self.activation_func == ActivationFunction.SIGMOID:\n",
        "\t\t\t\tde_dnet = Activations.d_sigmoid(self.last_unactivated) * (y-self.last_output)\n",
        "\t\t\telif self.activation_func == ActivationFunction.RELU:\n",
        "\t\t\t\tde_dnet = Activations.d_relu(self.last_unactivated) * (y-self.last_output)\n",
        "\t\t\telif self.activation_func == ActivationFunction.LINEAR:\n",
        "\t\t\t\tde_dnet = Activations.d_linear(self.last_unactivated) * (y-self.last_output)\n",
        "\t\t\telif self.activation_func == ActivationFunction.SOFTMAX:\n",
        "\t\t\t\tde_dnet = Activations.d_softmax(self.last_output, y)\n",
        "\t\telse:\n",
        "\t\t\tif self.activation_func == ActivationFunction.SIGMOID:\n",
        "\t\t\t\tde_dnet = delta * Activations.d_sigmoid(self.last_unactivated)\n",
        "\t\t\telif self.activation_func == ActivationFunction.RELU:\n",
        "\t\t\t\tde_dnet = delta * Activations.d_relu(self.last_unactivated)\n",
        "\t\t\telif self.activation_func == ActivationFunction.LINEAR:\n",
        "\t\t\t\tde_dnet = delta * Activations.d_linear(self.last_unactivated)\n",
        "\t\tcurr_derivative = de_dnet.T@self.last_input\n",
        "\t\tgrad_w = (de_dnet@self.weights.T)\n",
        "\t\tgrad_w = grad_w[:,1:]\n",
        "\t\tself.weights += lr*curr_derivative.T\n",
        "\t\treturn grad_w\n",
        "\n",
        "\tdef display_table(self):\n",
        "\t\tprint(tabulate([[\"Weights\", self.weights], [\"Activation\", self.activation_func]], tablefmt='pretty'))"
      ],
      "metadata": {
        "id": "QC9FJsPWGu57"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Class"
      ],
      "metadata": {
        "id": "hTc9FvzzG7O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph:\n",
        "\tdef __init__(self, input_count, n_layers, n_neurons, activation_funcs, lr, err_thresh, batch_size, max_iter=10000, print_per_iter=1000):\n",
        "\t\tself.layers = []\n",
        "\t\tself.lr = lr\n",
        "\t\tself.err_thresh = err_thresh\n",
        "\t\tself.max_iter = max_iter\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.n_layer = n_layers\n",
        "\t\tself.print_per_iter = print_per_iter\n",
        "\t\tfor i in range(n_layers):\n",
        "\t\t\tactivation_func = activation_funcs[i]\n",
        "\t\t\tif (n_neurons[i] is not None):\n",
        "\t\t\t\tn_neuron = n_neurons[i]\n",
        "\t\t\telse: \n",
        "\t\t\t\tn_neuron = 3\n",
        "\t\t\tif (i>0):\n",
        "\t\t\t\tself.layers.append(Layer(self.layers[i-1].num_nodes, n_neuron, activation_func))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.layers.append(Layer(input_count, n_neuron, activation_func))\n",
        "\t\t\tself.output_activation = self.layers[-1].activation_func\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn \"Graph with {} layers\".format(len(self.layers))\n",
        "\t\n",
        "\tdef add_layer(self, layer: Layer):\n",
        "\t\tself.layers.append(layer)\n",
        "\n",
        "\tdef predict(self, x: np.ndarray):\n",
        "\t\th = x\n",
        "\t\tfor i,l in enumerate(self.layers):\n",
        "\t\t\th = l.forward(h)\n",
        "\t\treturn h\n",
        "\t\t\t\n",
        "\tdef loss(self, yhat: np.ndarray, y: np.ndarray):\n",
        "\t\t\treturn np.sum(np.square(y-yhat))/2\n",
        "\n",
        "\tdef softmax_loss(self, yhat: np.ndarray, y: np.ndarray):\n",
        "\t\t\treturn -np.log(yhat[np.arange(yhat.shape[0]), y.flatten()]).sum()/yhat.shape[0]\n",
        "\n",
        "\tdef train(self, x_train: np.ndarray, y_train: np.ndarray):\n",
        "\t\tcount_iter = 0\n",
        "\t\twhile 1:\n",
        "\t\t\terr = 0\n",
        "\t\t\tfor batch in range((x_train.shape[0]//self.batch_size)+(1 if x_train.shape[0]%self.batch_size > 0 else 0)):\n",
        "\t\t\t\tlower = batch*self.batch_size\n",
        "\t\t\t\tupper = (batch+1)*self.batch_size\n",
        "\t\t\t\tbatch_x = x_train[lower:upper,:]\n",
        "\t\t\t\tbatch_y = y_train[lower:upper,:]\n",
        "\n",
        "\t\t\t\tyhat = self.predict(batch_x)\n",
        "\n",
        "\t\t\t\tif (self.output_activation == ActivationFunction.SOFTMAX):\n",
        "\t\t\t\t\terr += self.softmax_loss(yhat, batch_y)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\terr += self.loss(yhat, batch_y)\n",
        "\n",
        "\n",
        "\t\t\t\tdelta = self.layers[-1].backpropagate(None, self.lr, batch_y)\n",
        "\t\t\t\tfor i, l in enumerate(self.layers[-2::-1]):\n",
        "\t\t\t\t\tdelta = l.backpropagate(delta, self.lr)\n",
        "\t\t\tcount_iter+= 1\n",
        "\t\t\tif count_iter % self.print_per_iter == 0:\n",
        "\t\t\t\tprint(f'Iteration {count_iter}: error {err:.7f}')\n",
        "\t\t\tif count_iter >= self.max_iter:\n",
        "\t\t\t\tprint(\"Reached max iteration\")\n",
        "\t\t\t\tprint(f'Ended at iteration: {count_iter}')\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif err <= self.err_thresh:\n",
        "\t\t\t\tprint(\"Reached error below threshold\")\n",
        "\t\t\t\tprint(f'Ended at iteration: {count_iter}')\n",
        "\t\tprint(f'Final error: {err}')\n",
        "\t\t\n",
        "\tdef display_table(self):\n",
        "\t\ttrainable = 0\n",
        "\t\tfor i,l in enumerate(self.layers):\n",
        "\t\t\tprint(f'Layer {i}:')\n",
        "\t\t\tl.display_table()\n",
        "\t\t\tprint()\n",
        "\t\t\ttrainable += l.get_params()\n",
        "\t\tprint(tabulate([[\"Number of hidden layers: \", self.n_layer - 1], ['Total trainable params: ', trainable]], tablefmt='pretty'))\n"
      ],
      "metadata": {
        "id": "A_vqOVy7G-Gn"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Function"
      ],
      "metadata": {
        "id": "UQK0SFh3HHra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "graf = Graph(len(iris.feature_names), 2, [3, len(iris.target_names)], ['sigmoid', 'softmax'], 1e-2, 2e-2,50)\n",
        "graf.train(x, y.reshape(-1,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUBdZGY5ST3a",
        "outputId": "165910a5-4ea6-4ee0-c6e2-29d46b844619"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000: error 1.7438625\n",
            "Iteration 2000: error 1.6974770\n",
            "Iteration 3000: error 1.6943078\n",
            "Iteration 4000: error 1.6934626\n",
            "Iteration 5000: error 1.6922446\n",
            "Iteration 6000: error 1.6686615\n",
            "Iteration 7000: error 0.3037951\n",
            "Iteration 8000: error 0.2557990\n",
            "Iteration 9000: error 0.2462458\n",
            "Iteration 10000: error 0.2458087\n",
            "Reached max iteration\n",
            "Ended at iteration: 10000\n",
            "Final error: 0.24580868785853754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_1 = graf.predict(x)\n",
        "yhat = np.argmax(yhat_1, axis=1)\n",
        "print(iris.target_names[yhat])"
      ],
      "metadata": {
        "id": "y-QC9pSPHJTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a421f5f-f58c-4b02-9f97-e16213b67e71"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
            " 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
            " 'versicolor' 'virginica' 'versicolor' 'virginica' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
            " 'virginica' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
            " 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
            " 'virginica' 'virginica' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graf.display_table()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv5SOIoWSzzz",
        "outputId": "e55e2f9d-e0db-4279-c527-e78c6e9160de"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0:\n",
            "+------------+--------------------------------------------+\n",
            "|  Weights   | [[  0.4957162   11.98815757  -0.43667087]  |\n",
            "|            |  [  0.57388781   9.68238803  -0.92292776]  |\n",
            "|            |  [  3.3749871    9.46174545  -1.8931384 ]  |\n",
            "|            |  [ -3.94710594 -16.29427717   0.82437936]  |\n",
            "|            |  [ -5.18296249 -13.96861802  -1.37772225]] |\n",
            "| Activation |                  sigmoid                   |\n",
            "+------------+--------------------------------------------+\n",
            "\n",
            "Layer 1:\n",
            "+------------+-----------------------------------------+\n",
            "|  Weights   | [[-5.16465071  0.99555748  4.16909323]  |\n",
            "|            |  [13.95311871 -2.48011453 -9.66990733]  |\n",
            "|            |  [ 2.40827307  4.53929817 -6.9025371 ]  |\n",
            "|            |  [-2.51977239 -0.02532598  0.02571495]] |\n",
            "| Activation |                 softmax                 |\n",
            "+------------+-----------------------------------------+\n",
            "\n",
            "+--------------------------+----+\n",
            "| Number of hidden layers: | 1  |\n",
            "| Total trainable params:  | 27 |\n",
            "+--------------------------+----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pS3zts5LXalQ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S4C_OMI6SfFn"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}